{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required modules\n",
      "findspark  found\n",
      "pyspark  found\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from subprocess import check_call\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "modules = [\"findspark\",\"pyspark\"]\n",
    "\n",
    "def check_modules(modules, upgrade=False):\n",
    "    print(\"Checking required modules\")\n",
    "    for m in modules:\n",
    "        torch_loader = importlib.util.find_spec(m)\n",
    "        if torch_loader is not None and not upgrade:\n",
    "            print(m,\" found\")\n",
    "        else:\n",
    "            if upgrade:\n",
    "                print(\"upgrading \",m)\n",
    "            else:\n",
    "                print(m,\" not found, installing\")\n",
    "            if 'google.colab' in sys.modules:\n",
    "                if upgrade:\n",
    "                    check_call([\"pip\", \"install\", \"--upgrade\", m])\n",
    "                else:\n",
    "                    check_call([\"pip\", \"install\", \"-q\", m])\n",
    "            else:\n",
    "                if upgrade:\n",
    "                    check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--upgrade\", m])\n",
    "                else:\n",
    "                    check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", m])\n",
    "\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    spark = \"spark-3.2.0-bin-hadoop3.2.tgz\"\n",
    "    if not os.path.isfile(spark):\n",
    "        !wget -q https://downloads.apache.org/spark/spark-3.2.0/{spark}\n",
    "        !tar xf {spark}\n",
    "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "        os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
    "        \n",
    "check_modules(modules)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hecho\n",
      "Preparado!!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "# cambiamos las variables del sistema\n",
    "spark = 'C:/ucm-master-iot/TDM/spark/spark-3.1.2-bin-hadoop3.2'\n",
    "#spark = 'C:/hlocal/tdm/spark/spark-3.2.0-bin-hadoop3.2'\n",
    "if not(os.path.isdir(spark+\"/bin\")) or not(os.path.isdir(spark+\"/jars\")) :\n",
    "        print(\"Error, la carpeta en 'spark' debe contener los directorios bin y jars \")\n",
    "else:    \n",
    "        # en el path se añade\n",
    "        #path = os.environ.get('PATH') \n",
    "        #path = path+ ';'+spark+'\\\\bin;'\n",
    "        #os.environ['PATH'] = path\n",
    "        os.environ['SPARK_HOME']= spark \n",
    "        os.environ['HADOOP_HOME']= spark \n",
    "        os.environ['PYSPARK_DRIVER_PYTHON']= 'jupyter'\n",
    "        os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
    "\n",
    "        # si da problema con collect quizás haya que poner java_home a la localización de java 8\n",
    "        #os.environ['JAVA_HOME']= 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151'\n",
    "        #os.environ['PATH'] = os.environ.get('JAVA_HOME')+'\\\\bin;'+spark\n",
    "        print(\"Hecho\")\n",
    "print(\"Preparado!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   hi|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() # SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "df = spark.sql('''select 'spark' as hi ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| items|freq|\n",
      "+------+----+\n",
      "|   [B]|   6|\n",
      "|   [D]|   5|\n",
      "|[D, B]|   4|\n",
      "|   [C]|   4|\n",
      "|[C, D]|   3|\n",
      "|[C, B]|   3|\n",
      "|   [A]|   3|\n",
      "|[A, B]|   3|\n",
      "+------+----+\n",
      "\n",
      "+----------+----------+----------+------------------+-------------------+\n",
      "|antecedent|consequent|confidence|              lift|            support|\n",
      "+----------+----------+----------+------------------+-------------------+\n",
      "|       [C]|       [D]|      0.75|              1.05|0.42857142857142855|\n",
      "|       [C]|       [B]|      0.75|             0.875|0.42857142857142855|\n",
      "|       [A]|       [B]|       1.0|1.1666666666666667|0.42857142857142855|\n",
      "|       [D]|       [B]|       0.8|0.9333333333333335| 0.5714285714285714|\n",
      "+----------+----------+----------+------------------+-------------------+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|[A, B, C, D]|        []|\n",
      "|  1|   [A, B, D]|        []|\n",
      "|  2|      [A, B]|        []|\n",
      "|  3|   [B, C, D]|        []|\n",
      "|  4|      [B, C]|       [D]|\n",
      "|  5|      [C, D]|       [B]|\n",
      "|  6|      [B, D]|        []|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, ['A','B','C','D']),\n",
    "    (1, ['A','B','D']),\n",
    "    (2, ['A','B']),\n",
    "    (3, ['B','C','D']),\n",
    "    (4, ['B','C']),\n",
    "    (5, ['C','D']),\n",
    "    (6, ['B','D'])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.4, minConfidence=0.7)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules\n",
    "# and summarize the consequents as prediction\n",
    "model.transform(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
